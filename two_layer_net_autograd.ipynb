{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors and autograd\n",
    "-------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Tensors, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "\n",
    "A PyTorch Tensor represents a node in a computational graph. If ``x`` is a\n",
    "Tensor that has ``x.requires_grad=True`` then ``x.grad`` is another Tensor\n",
    "holding the gradient of ``x`` with respect to some scalar value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27392828.0\n",
      "1 21725772.0\n",
      "2 19557388.0\n",
      "3 17959060.0\n",
      "4 15690210.0\n",
      "5 12682205.0\n",
      "6 9418000.0\n",
      "7 6563860.5\n",
      "8 4401853.0\n",
      "9 2931817.0\n",
      "10 1984665.0\n",
      "11 1388778.875\n",
      "12 1011713.8125\n",
      "13 767774.5\n",
      "14 604510.75\n",
      "15 490833.625\n",
      "16 408272.6875\n",
      "17 345919.28125\n",
      "18 297254.0625\n",
      "19 258162.859375\n",
      "20 226051.953125\n",
      "21 199197.84375\n",
      "22 176461.390625\n",
      "23 157005.359375\n",
      "24 140213.359375\n",
      "25 125641.7109375\n",
      "26 112906.1171875\n",
      "27 101723.6484375\n",
      "28 91882.171875\n",
      "29 83175.171875\n",
      "30 75443.6640625\n",
      "31 68562.875\n",
      "32 62422.5390625\n",
      "33 56926.625\n",
      "34 52005.12109375\n",
      "35 47589.1015625\n",
      "36 43608.86328125\n",
      "37 40014.1171875\n",
      "38 36763.0390625\n",
      "39 33817.91796875\n",
      "40 31143.642578125\n",
      "41 28712.490234375\n",
      "42 26499.80078125\n",
      "43 24482.0390625\n",
      "44 22639.513671875\n",
      "45 20955.240234375\n",
      "46 19414.390625\n",
      "47 18003.05859375\n",
      "48 16708.875\n",
      "49 15519.9599609375\n",
      "50 14427.005859375\n",
      "51 13421.013671875\n",
      "52 12494.1669921875\n",
      "53 11639.5537109375\n",
      "54 10850.87109375\n",
      "55 10122.328125\n",
      "56 9448.8212890625\n",
      "57 8825.6640625\n",
      "58 8249.470703125\n",
      "59 7715.84228515625\n",
      "60 7220.79638671875\n",
      "61 6761.30029296875\n",
      "62 6334.458984375\n",
      "63 5938.8935546875\n",
      "64 5571.2314453125\n",
      "65 5228.9716796875\n",
      "66 4910.181640625\n",
      "67 4613.02490234375\n",
      "68 4335.85546875\n",
      "69 4077.284912109375\n",
      "70 3835.9375\n",
      "71 3610.44287109375\n",
      "72 3399.774169921875\n",
      "73 3202.79931640625\n",
      "74 3018.441162109375\n",
      "75 2845.79931640625\n",
      "76 2684.114990234375\n",
      "77 2532.53369140625\n",
      "78 2390.50537109375\n",
      "79 2257.251953125\n",
      "80 2132.18359375\n",
      "81 2014.743896484375\n",
      "82 1904.429931640625\n",
      "83 1800.71435546875\n",
      "84 1703.18896484375\n",
      "85 1611.4508056640625\n",
      "86 1525.1307373046875\n",
      "87 1443.84130859375\n",
      "88 1367.287109375\n",
      "89 1295.1790771484375\n",
      "90 1227.1951904296875\n",
      "91 1163.11181640625\n",
      "92 1102.688720703125\n",
      "93 1045.6732177734375\n",
      "94 991.8600463867188\n",
      "95 941.0528564453125\n",
      "96 893.0819702148438\n",
      "97 847.7446899414062\n",
      "98 804.8931274414062\n",
      "99 764.3793334960938\n",
      "100 726.0728759765625\n",
      "101 689.835205078125\n",
      "102 655.552001953125\n",
      "103 623.09814453125\n",
      "104 592.37158203125\n",
      "105 563.271240234375\n",
      "106 535.711669921875\n",
      "107 509.595458984375\n",
      "108 484.8427734375\n",
      "109 461.38165283203125\n",
      "110 439.13323974609375\n",
      "111 418.0293273925781\n",
      "112 398.0130615234375\n",
      "113 379.019775390625\n",
      "114 360.99639892578125\n",
      "115 343.89007568359375\n",
      "116 327.6440734863281\n",
      "117 312.2112731933594\n",
      "118 297.5526123046875\n",
      "119 283.62518310546875\n",
      "120 270.38848876953125\n",
      "121 257.80670166015625\n",
      "122 245.8465118408203\n",
      "123 234.478759765625\n",
      "124 223.66241455078125\n",
      "125 213.37203979492188\n",
      "126 203.5822296142578\n",
      "127 194.26739501953125\n",
      "128 185.40219116210938\n",
      "129 176.963134765625\n",
      "130 168.92718505859375\n",
      "131 161.27627563476562\n",
      "132 153.99575805664062\n",
      "133 147.0701904296875\n",
      "134 140.47222900390625\n",
      "135 134.18563842773438\n",
      "136 128.19581604003906\n",
      "137 122.4884033203125\n",
      "138 117.045166015625\n",
      "139 111.85514831542969\n",
      "140 106.90612030029297\n",
      "141 102.18545532226562\n",
      "142 97.68266296386719\n",
      "143 93.38890075683594\n",
      "144 89.29153442382812\n",
      "145 85.38182067871094\n",
      "146 81.65121459960938\n",
      "147 78.09004211425781\n",
      "148 74.69110107421875\n",
      "149 71.44514465332031\n",
      "150 68.34752655029297\n",
      "151 65.39051818847656\n",
      "152 62.56483459472656\n",
      "153 59.8657112121582\n",
      "154 57.288387298583984\n",
      "155 54.825660705566406\n",
      "156 52.47340774536133\n",
      "157 50.22489929199219\n",
      "158 48.077754974365234\n",
      "159 46.024864196777344\n",
      "160 44.06267547607422\n",
      "161 42.18717956542969\n",
      "162 40.39410400390625\n",
      "163 38.679893493652344\n",
      "164 37.04156494140625\n",
      "165 35.475364685058594\n",
      "166 33.976524353027344\n",
      "167 32.54326248168945\n",
      "168 31.17226219177246\n",
      "169 29.861602783203125\n",
      "170 28.60732650756836\n",
      "171 27.406742095947266\n",
      "172 26.259418487548828\n",
      "173 25.16044807434082\n",
      "174 24.109325408935547\n",
      "175 23.103187561035156\n",
      "176 22.14034652709961\n",
      "177 21.21906280517578\n",
      "178 20.33721160888672\n",
      "179 19.493297576904297\n",
      "180 18.68463897705078\n",
      "181 17.910690307617188\n",
      "182 17.169971466064453\n",
      "183 16.460132598876953\n",
      "184 15.781001091003418\n",
      "185 15.130230903625488\n",
      "186 14.507713317871094\n",
      "187 13.910478591918945\n",
      "188 13.338701248168945\n",
      "189 12.791767120361328\n",
      "190 12.267595291137695\n",
      "191 11.764776229858398\n",
      "192 11.283576011657715\n",
      "193 10.822646141052246\n",
      "194 10.380544662475586\n",
      "195 9.957193374633789\n",
      "196 9.551441192626953\n",
      "197 9.16280746459961\n",
      "198 8.790029525756836\n",
      "199 8.432914733886719\n",
      "200 8.09067153930664\n",
      "201 7.76253604888916\n",
      "202 7.448326587677002\n",
      "203 7.146745204925537\n",
      "204 6.85788631439209\n",
      "205 6.580723762512207\n",
      "206 6.3150835037231445\n",
      "207 6.060625076293945\n",
      "208 5.8162689208984375\n",
      "209 5.582202911376953\n",
      "210 5.357425689697266\n",
      "211 5.142192840576172\n",
      "212 4.935581207275391\n",
      "213 4.737842559814453\n",
      "214 4.547649383544922\n",
      "215 4.3656511306762695\n",
      "216 4.190845489501953\n",
      "217 4.023437023162842\n",
      "218 3.86256742477417\n",
      "219 3.7084803581237793\n",
      "220 3.560446262359619\n",
      "221 3.4185266494750977\n",
      "222 3.282465934753418\n",
      "223 3.1516242027282715\n",
      "224 3.0264058113098145\n",
      "225 2.906083106994629\n",
      "226 2.7907333374023438\n",
      "227 2.679914712905884\n",
      "228 2.573734760284424\n",
      "229 2.471707820892334\n",
      "230 2.373741865158081\n",
      "231 2.2798869609832764\n",
      "232 2.1896653175354004\n",
      "233 2.103109359741211\n",
      "234 2.020034074783325\n",
      "235 1.9403749704360962\n",
      "236 1.864020824432373\n",
      "237 1.7905776500701904\n",
      "238 1.7199389934539795\n",
      "239 1.6523289680480957\n",
      "240 1.5873783826828003\n",
      "241 1.5249451398849487\n",
      "242 1.4650938510894775\n",
      "243 1.4075514078140259\n",
      "244 1.3522963523864746\n",
      "245 1.299215316772461\n",
      "246 1.2483534812927246\n",
      "247 1.1995227336883545\n",
      "248 1.1525719165802002\n",
      "249 1.1075489521026611\n",
      "250 1.0641860961914062\n",
      "251 1.0226856470108032\n",
      "252 0.9827532768249512\n",
      "253 0.9444122314453125\n",
      "254 0.9076031446456909\n",
      "255 0.8722591400146484\n",
      "256 0.838286280632019\n",
      "257 0.8056786060333252\n",
      "258 0.7742899656295776\n",
      "259 0.7442548274993896\n",
      "260 0.7152822017669678\n",
      "261 0.6874976754188538\n",
      "262 0.6608858108520508\n",
      "263 0.6352857351303101\n",
      "264 0.610693633556366\n",
      "265 0.5869594812393188\n",
      "266 0.5641825795173645\n",
      "267 0.5423584580421448\n",
      "268 0.521378755569458\n",
      "269 0.501228392124176\n",
      "270 0.48184874653816223\n",
      "271 0.4632699191570282\n",
      "272 0.44538775086402893\n",
      "273 0.42820048332214355\n",
      "274 0.4117048978805542\n",
      "275 0.39587509632110596\n",
      "276 0.3806273639202118\n",
      "277 0.3660077452659607\n",
      "278 0.3519101142883301\n",
      "279 0.33837780356407166\n",
      "280 0.325374037027359\n",
      "281 0.3128530979156494\n",
      "282 0.3009116053581238\n",
      "283 0.28929024934768677\n",
      "284 0.27819180488586426\n",
      "285 0.2675405740737915\n",
      "286 0.25727251172065735\n",
      "287 0.24745069444179535\n",
      "288 0.23797360062599182\n",
      "289 0.2288426160812378\n",
      "290 0.2201044261455536\n",
      "291 0.2117050588130951\n",
      "292 0.20364002883434296\n",
      "293 0.19584713876247406\n",
      "294 0.1883832961320877\n",
      "295 0.18118742108345032\n",
      "296 0.1742667257785797\n",
      "297 0.16762691736221313\n",
      "298 0.16122274100780487\n",
      "299 0.15510836243629456\n",
      "300 0.1492117941379547\n",
      "301 0.14352908730506897\n",
      "302 0.13803602755069733\n",
      "303 0.1328115016222\n",
      "304 0.12775203585624695\n",
      "305 0.12292934954166412\n",
      "306 0.11826039850711823\n",
      "307 0.11377843469381332\n",
      "308 0.10946884006261826\n",
      "309 0.10530231893062592\n",
      "310 0.10129845887422562\n",
      "311 0.0974869430065155\n",
      "312 0.09379808604717255\n",
      "313 0.09025221318006516\n",
      "314 0.08684442937374115\n",
      "315 0.08356079459190369\n",
      "316 0.08040334284305573\n",
      "317 0.07737421244382858\n",
      "318 0.07446012645959854\n",
      "319 0.07163914293050766\n",
      "320 0.06893595308065414\n",
      "321 0.06634607911109924\n",
      "322 0.0638497918844223\n",
      "323 0.06142649054527283\n",
      "324 0.05911668762564659\n",
      "325 0.05690177530050278\n",
      "326 0.0547608956694603\n",
      "327 0.052712783217430115\n",
      "328 0.05072275549173355\n",
      "329 0.0488225556910038\n",
      "330 0.046983037143945694\n",
      "331 0.04523966833949089\n",
      "332 0.04354376718401909\n",
      "333 0.04191306233406067\n",
      "334 0.04033290594816208\n",
      "335 0.03883134573698044\n",
      "336 0.03738786280155182\n",
      "337 0.03598850965499878\n",
      "338 0.03463968634605408\n",
      "339 0.03335264325141907\n",
      "340 0.03210477530956268\n",
      "341 0.030910860747098923\n",
      "342 0.029759764671325684\n",
      "343 0.028662867844104767\n",
      "344 0.0275996383279562\n",
      "345 0.026577863842248917\n",
      "346 0.02558722347021103\n",
      "347 0.0246441587805748\n",
      "348 0.023733951151371002\n",
      "349 0.022855278104543686\n",
      "350 0.02200240269303322\n",
      "351 0.02119806967675686\n",
      "352 0.020414691418409348\n",
      "353 0.019661404192447662\n",
      "354 0.01894243061542511\n",
      "355 0.018249928951263428\n",
      "356 0.0175764299929142\n",
      "357 0.016931995749473572\n",
      "358 0.01631072163581848\n",
      "359 0.0157182477414608\n",
      "360 0.0151411984115839\n",
      "361 0.014587691985070705\n",
      "362 0.01405204739421606\n",
      "363 0.013548780232667923\n",
      "364 0.013057442381978035\n",
      "365 0.012581001967191696\n",
      "366 0.012127643451094627\n",
      "367 0.011687302961945534\n",
      "368 0.011260994710028172\n",
      "369 0.010862467810511589\n",
      "370 0.010469590313732624\n",
      "371 0.010097809135913849\n",
      "372 0.00973113626241684\n",
      "373 0.009383002296090126\n",
      "374 0.009048224426805973\n",
      "375 0.00872481893748045\n",
      "376 0.008413007482886314\n",
      "377 0.008117089048027992\n",
      "378 0.007827294990420341\n",
      "379 0.007551340386271477\n",
      "380 0.007290803361684084\n",
      "381 0.007028820924460888\n",
      "382 0.006785321049392223\n",
      "383 0.006546076387166977\n",
      "384 0.006317924242466688\n",
      "385 0.0060937292873859406\n",
      "386 0.0058889202773571014\n",
      "387 0.0056848241947591305\n",
      "388 0.005487773101776838\n",
      "389 0.005298025906085968\n",
      "390 0.005116463638842106\n",
      "391 0.004942178726196289\n",
      "392 0.004773440770804882\n",
      "393 0.0046111117117106915\n",
      "394 0.004454757086932659\n",
      "395 0.004302068147808313\n",
      "396 0.004155106842517853\n",
      "397 0.004016389138996601\n",
      "398 0.0038838316686451435\n",
      "399 0.0037493896670639515\n",
      "400 0.003626637626439333\n",
      "401 0.0035051293671131134\n",
      "402 0.0033904751762747765\n",
      "403 0.0032794885337352753\n",
      "404 0.00317012588493526\n",
      "405 0.0030661995988339186\n",
      "406 0.00296710804104805\n",
      "407 0.0028740898706018925\n",
      "408 0.002781426999717951\n",
      "409 0.002691832836717367\n",
      "410 0.002604224719107151\n",
      "411 0.002521457616239786\n",
      "412 0.002443657722324133\n",
      "413 0.0023663784377276897\n",
      "414 0.0022946211975067854\n",
      "415 0.0022217407822608948\n",
      "416 0.0021547237411141396\n",
      "417 0.0020881127566099167\n",
      "418 0.0020223059691488743\n",
      "419 0.0019607562571763992\n",
      "420 0.00190222286619246\n",
      "421 0.0018444594461470842\n",
      "422 0.0017877377104014158\n",
      "423 0.0017349267145618796\n",
      "424 0.001682585570961237\n",
      "425 0.0016349670477211475\n",
      "426 0.0015859048580750823\n",
      "427 0.001540636527352035\n",
      "428 0.0014959152322262526\n",
      "429 0.0014536186354234815\n",
      "430 0.0014109252952039242\n",
      "431 0.0013707884354516864\n",
      "432 0.0013308590278029442\n",
      "433 0.0012921279994770885\n",
      "434 0.001255313865840435\n",
      "435 0.001221159240230918\n",
      "436 0.0011871871538460255\n",
      "437 0.0011542197316884995\n",
      "438 0.001124837319366634\n",
      "439 0.0010921021457761526\n",
      "440 0.001062772236764431\n",
      "441 0.0010340972803533077\n",
      "442 0.0010061385110020638\n",
      "443 0.0009792158380150795\n",
      "444 0.0009527173824608326\n",
      "445 0.0009282181272283196\n",
      "446 0.0009045536862686276\n",
      "447 0.0008806838886812329\n",
      "448 0.0008583180606365204\n",
      "449 0.0008369798306375742\n",
      "450 0.0008167097112163901\n",
      "451 0.0007946784025989473\n",
      "452 0.0007756081758998334\n",
      "453 0.0007549235597252846\n",
      "454 0.0007374179549515247\n",
      "455 0.0007185740396380424\n",
      "456 0.0007023958023637533\n",
      "457 0.0006844702875241637\n",
      "458 0.0006681173108518124\n",
      "459 0.000652097340207547\n",
      "460 0.0006355978548526764\n",
      "461 0.0006218404741957784\n",
      "462 0.0006077677244320512\n",
      "463 0.0005933435168117285\n",
      "464 0.0005800137296319008\n",
      "465 0.0005664891796186566\n",
      "466 0.000553276389837265\n",
      "467 0.0005409432924352586\n",
      "468 0.0005287195672281086\n",
      "469 0.0005166104529052973\n",
      "470 0.0005052066408097744\n",
      "471 0.0004940772778354585\n",
      "472 0.0004832828708458692\n",
      "473 0.00047177524538710713\n",
      "474 0.00046190511784516275\n",
      "475 0.00045167835196480155\n",
      "476 0.00044214370427653193\n",
      "477 0.00043302058475092053\n",
      "478 0.00042288261465728283\n",
      "479 0.00041389590478502214\n",
      "480 0.0004044075030833483\n",
      "481 0.00039586288039572537\n",
      "482 0.0003883316821884364\n",
      "483 0.0003804318839684129\n",
      "484 0.0003725917194969952\n",
      "485 0.00036504684248939157\n",
      "486 0.0003579897165764123\n",
      "487 0.00035074871266260743\n",
      "488 0.00034364438033662736\n",
      "489 0.0003366865566931665\n",
      "490 0.0003300202661193907\n",
      "491 0.00032388605177402496\n",
      "492 0.00031761854188516736\n",
      "493 0.00031104785739444196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494 0.00030520028667524457\n",
      "495 0.00029992428608238697\n",
      "496 0.0002939214464277029\n",
      "497 0.0002890592732001096\n",
      "498 0.00028342934092506766\n",
      "499 0.0002776745823211968\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    \n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
